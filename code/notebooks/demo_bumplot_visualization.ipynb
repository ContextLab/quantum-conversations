{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bumplot Visualization Demo\n",
    "\n",
    "This notebook demonstrates the new bumplot visualization feature for particle trajectories in Quantum Conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from quantum_conversations import ParticleFilter, TokenSequenceVisualizer, ModelManager\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Bumplot Visualization\n",
    "\n",
    "Generate particles and visualize their trajectories using a bump plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize particle filter with a small model\n",
    "model_manager = ModelManager()\n",
    "pf = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=10,\n",
    "    temperature=1.0,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Generate particles\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "particles = pf.generate(prompt, max_new_tokens=20)\n",
    "\n",
    "print(f\"Generated {len(particles)} particles\")\n",
    "print(f\"Each particle has {len(particles[0].token_ids)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bumplot visualization with probability coloring\n",
    "viz = TokenSequenceVisualizer(tokenizer=pf.tokenizer)\n",
    "fig = viz.visualize_bumplot(\n",
    "    particles,\n",
    "    color_by='transition_prob',\n",
    "    max_vocab_display=50,\n",
    "    show_tokens=False,\n",
    "    figsize=(14, 8)\n",
    ")\n",
    "plt.title(f\"Bumplot: Token Trajectories for '{prompt}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Different Color Schemes\n",
    "\n",
    "Visualize the same particles with different coloring strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate particles with higher temperature for more diversity\n",
    "pf_diverse = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=8,\n",
    "    temperature=1.5,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "prompt_diverse = \"Once upon a time\"\n",
    "particles_diverse = pf_diverse.generate(prompt_diverse, max_new_tokens=15)\n",
    "\n",
    "viz_diverse = TokenSequenceVisualizer(tokenizer=pf_diverse.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for different color schemes\n",
    "color_schemes = ['transition_prob', 'entropy', 'particle_id']\n",
    "titles = ['Colored by Transition Probability', 'Colored by Entropy', 'Colored by Particle ID']\n",
    "\n",
    "for i, (color_by, title) in enumerate(zip(color_schemes, titles)):\n",
    "    fig = viz_diverse.visualize_bumplot(\n",
    "        particles_diverse,\n",
    "        color_by=color_by,\n",
    "        max_vocab_display=30,\n",
    "        show_tokens=False,\n",
    "        figsize=(12, 6)\n",
    "    )\n",
    "    plt.title(f\"{title}\\nPrompt: '{prompt_diverse}'\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"\\n{title} visualization complete\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convergence vs Divergence Patterns\n",
    "\n",
    "Compare low and high temperature generation to see convergence and divergence patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low temperature for convergence\n",
    "pf_converge = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=10,\n",
    "    temperature=0.2,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# High temperature for divergence\n",
    "pf_diverge = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=10,\n",
    "    temperature=2.0,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "prompt_test = \"The answer to life is\"\n",
    "particles_converge = pf_converge.generate(prompt_test, max_new_tokens=15)\n",
    "particles_diverge = pf_diverge.generate(prompt_test, max_new_tokens=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence (low temperature)\n",
    "viz_converge = TokenSequenceVisualizer(tokenizer=pf_converge.tokenizer)\n",
    "fig = viz_converge.visualize_bumplot(\n",
    "    particles_converge,\n",
    "    color_by='particle_id',\n",
    "    max_vocab_display=40,\n",
    "    show_tokens=False,\n",
    "    figsize=(14, 7)\n",
    ")\n",
    "plt.title(f\"Low Temperature (0.2) - Convergent Behavior\\nPrompt: '{prompt_test}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize divergence (high temperature)\n",
    "viz_diverge = TokenSequenceVisualizer(tokenizer=pf_diverge.tokenizer)\n",
    "fig = viz_diverge.visualize_bumplot(\n",
    "    particles_diverge,\n",
    "    color_by='particle_id',\n",
    "    max_vocab_display=40,\n",
    "    show_tokens=False,\n",
    "    figsize=(14, 7)\n",
    ")\n",
    "plt.title(f\"High Temperature (2.0) - Divergent Behavior\\nPrompt: '{prompt_test}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. With Token Labels\n",
    "\n",
    "Show actual token text on the bump plot for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shorter sequence for clearer token labels\n",
    "pf_labels = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=6,\n",
    "    temperature=0.8,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "prompt_short = \"Hello world\"\n",
    "particles_short = pf_labels.generate(prompt_short, max_new_tokens=8)\n",
    "\n",
    "viz_labels = TokenSequenceVisualizer(tokenizer=pf_labels.tokenizer)\n",
    "fig = viz_labels.visualize_bumplot(\n",
    "    particles_short,\n",
    "    color_by='transition_prob',\n",
    "    max_vocab_display=15,  # Limit for clearer labels\n",
    "    show_tokens=True,  # Show token text\n",
    "    figsize=(14, 8)\n",
    ")\n",
    "plt.title(f\"Bumplot with Token Labels\\nPrompt: '{prompt_short}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Larger Model Comparison\n",
    "\n",
    "Compare particle trajectories from different model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small model\n",
    "pf_small = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=6,\n",
    "    temperature=1.0,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Larger model (GPT-2)\n",
    "pf_large = ParticleFilter(\n",
    "    model_name=\"gpt2\",\n",
    "    n_particles=6,\n",
    "    temperature=1.0,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "prompt_compare = \"In the beginning\"\n",
    "particles_small_model = pf_small.generate(prompt_compare, max_new_tokens=12)\n",
    "particles_large_model = pf_large.generate(prompt_compare, max_new_tokens=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize small model\n",
    "viz_small = TokenSequenceVisualizer(tokenizer=pf_small.tokenizer)\n",
    "fig = viz_small.visualize_bumplot(\n",
    "    particles_small_model,\n",
    "    color_by='transition_prob',\n",
    "    max_vocab_display=30,\n",
    "    show_tokens=False,\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.title(f\"Pythia-70M Model\\nPrompt: '{prompt_compare}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize large model\n",
    "viz_large = TokenSequenceVisualizer(tokenizer=pf_large.tokenizer)\n",
    "fig = viz_large.visualize_bumplot(\n",
    "    particles_large_model,\n",
    "    color_by='transition_prob',\n",
    "    max_vocab_display=30,\n",
    "    show_tokens=False,\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.title(f\"GPT-2 Model\\nPrompt: '{prompt_compare}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined Visualizations\n",
    "\n",
    "Show bumplot alongside traditional Sankey diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate particles for combined visualization\n",
    "pf_combined = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=8,\n",
    "    temperature=0.9,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "prompt_combined = \"The meaning of life\"\n",
    "particles_combined = pf_combined.generate(prompt_combined, max_new_tokens=12)\n",
    "viz_combined = TokenSequenceVisualizer(tokenizer=pf_combined.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bumplot visualization\n",
    "print(\"Bumplot Visualization:\")\n",
    "print(\"=\"*50)\n",
    "fig_bump = viz_combined.visualize_bumplot(\n",
    "    particles_combined,\n",
    "    color_by='transition_prob',\n",
    "    max_vocab_display=30,\n",
    "    show_tokens=False,\n",
    "    figsize=(14, 7)\n",
    ")\n",
    "plt.title(f\"Bumplot: Token Trajectories\\nPrompt: '{prompt_combined}'\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSankey Diagram:\")\n",
    "print(\"=\"*50)\n",
    "# Traditional Sankey visualization\n",
    "fig_sankey = viz_combined.visualize(\n",
    "    particles_combined,\n",
    "    prompt_combined,\n",
    "    figsize=(14, 8)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis Metrics\n",
    "\n",
    "Extract and analyze metrics from the particle trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantum_conversations import compute_sequence_entropy, compute_divergence_score\n",
    "\n",
    "# Compute entropy for each particle\n",
    "entropies = []\n",
    "for particle in particles_combined:\n",
    "    entropy = compute_sequence_entropy(particle)\n",
    "    entropies.append(entropy)\n",
    "\n",
    "# Compute divergence score\n",
    "divergence = compute_divergence_score(particles_combined)\n",
    "\n",
    "print(f\"Particle Entropy Statistics:\")\n",
    "print(f\"  Mean entropy: {np.mean(entropies):.4f}\")\n",
    "print(f\"  Std entropy: {np.std(entropies):.4f}\")\n",
    "print(f\"  Min entropy: {np.min(entropies):.4f}\")\n",
    "print(f\"  Max entropy: {np.max(entropies):.4f}\")\n",
    "print(f\"\\nDivergence score: {divergence:.4f}\")\n",
    "\n",
    "# Plot entropy distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(entropies, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.xlabel('Sequence Entropy')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Particle Entropies')\n",
    "plt.axvline(np.mean(entropies), color='red', linestyle='--', label=f'Mean: {np.mean(entropies):.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the new bumplot visualization feature for Quantum Conversations:\n",
    "\n",
    "1. **Basic bumplot** showing token trajectories over time\n",
    "2. **Different color schemes** (probability, entropy, particle ID)\n",
    "3. **Convergence vs divergence** patterns with temperature control\n",
    "4. **Token labels** for interpretability\n",
    "5. **Model comparisons** between different sizes\n",
    "6. **Combined visualizations** with traditional Sankey diagrams\n",
    "7. **Quantitative metrics** for analyzing particle behavior\n",
    "\n",
    "The bumplot visualization provides a clear view of how different particles explore the token space over time, making it easier to understand the divergence and convergence patterns in language model generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Conversations: Visualizing Token Generation Paths\\n",
    "\\n",
    "This notebook demonstrates the particle filter approach to tracking and visualizing the \\\"paths not taken\\\" in language model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nsys.path.append('..')  # Add parent directory to path to import quantum_conversations\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Force matplotlib to use inline backend\nimport matplotlib\nmatplotlib.use('Agg')\n%matplotlib inline\n\nfrom quantum_conversations import ParticleFilter, TokenSequenceVisualizer\n\n# Set up matplotlib style\nplt.style.use('default')  # Use default style as seaborn-v0_8 might not be available\n\n# Check GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Particle Filter\\n",
    "\\n",
    "We'll use TinyLlama-1.1B for efficient generation while still getting meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize particle filter\nprint(\"Initializing particle filter...\")\nprint(\"Note: This will download the TinyLlama model (~2.2GB) on first run.\")\n\n# Use 1000 particles as requested\npf = ParticleFilter(\n    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    n_particles=1000,\n    temperature=1.0,\n    top_k=100,\n    top_p=0.95,\n    device=device\n)\n\nprint(\"Model loaded successfully!\")\n\n# Initialize visualizer with settings for many thin particles\nviz = TokenSequenceVisualizer(\n    tokenizer=pf.tokenizer,\n    figsize=(20, 12),\n    alpha=0.01,      # Very low alpha for 1000 overlapping paths\n    line_width=0.1   # Very thin lines\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Starter Sequences\\n",
    "\\n",
    "We'll use a variety of prompts with different levels of ambiguity to see how the model explores different paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define starter sequences with varying ambiguity\\n",
    "starter_sequences = [\\n",
    "    # High ambiguity - many possible continuations\\n",
    "    \\\"It all started when \\\",\\n",
    "    \\\"I couldn't believe that \\\",\\n",
    "    \\\"The most surprising thing was \\\",\\n",
    "    \\\"Nobody expected \\\",\\n",
    "    \\\"In the middle of the night, \\\",\\n",
    "    \\n",
    "    # Medium ambiguity - some constraints\\n",
    "    \\\"The scientific experiment revealed \\\",\\n",
    "    \\\"The recipe called for \\\",\\n",
    "    \\\"The weather forecast predicted \\\",\\n",
    "    \\\"The detective discovered \\\",\\n",
    "    \\\"The teacher explained that \\\",\\n",
    "    \\n",
    "    # Low ambiguity - more predictable\\n",
    "    \\\"Two plus two equals \\\",\\n",
    "    \\\"The capital of France is \\\",\\n",
    "    \\\"Water freezes at \\\",\\n",
    "    \\\"The sun rises in the \\\",\\n",
    "    \\\"Photosynthesis converts \\\",\\n",
    "    \\n",
    "    # Story beginnings\\n",
    "    \\\"Once upon a time, \\\",\\n",
    "    \\\"In a galaxy far, far away, \\\",\\n",
    "    \\\"It was a dark and stormy night when \\\",\\n",
    "    \\\"The door creaked open, revealing \\\",\\n",
    "    \\\"She had never seen anything like \\\",\\n",
    "    \\n",
    "    # Questions and philosophical\\n",
    "    \\\"The meaning of life is \\\",\\n",
    "    \\\"What would happen if \\\",\\n",
    "    \\\"The best way to \\\",\\n",
    "    \\\"The secret to happiness is \\\",\\n",
    "    \\\"Time travel would \\\",\\n",
    "    \\n",
    "    # Technical/Code\\n",
    "    \\\"def fibonacci(n):\\\\n    \\\",\\n",
    "    \\\"The algorithm works by \\\",\\n",
    "    \\\"To solve this problem, we \\\",\\n",
    "    \\\"The function returns \\\",\\n",
    "    \\\"class NeuralNetwork:\\\\n    def __init__(self):\\\\n        \\\"\\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Visualize Single Example\\n",
    "\\n",
    "Let's start with a detailed look at one example to understand the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate particles for one example\nexample_prompt = \"The most surprising thing was \"\nprint(f\"Generating particles for: '{example_prompt}'\\n\")\n\n# Generate with 20 steps as requested\nparticles = pf.generate(example_prompt, max_new_tokens=20)\n\n# Show the generated sequences\nprint(\"\\nGenerated sequences:\\n\")\nsequences = pf.get_token_sequences()\n# Show only first 5 sequences\nfor i, (text, log_prob) in enumerate(sequences[:5]):\n    print(f\"Particle {i+1}: {text}\")\n    print(f\"  Log probability: {log_prob:.3f}\\n\")\n    \nprint(f\"... and {len(sequences)-5} more sequences\")\n\n# Find most probable sequence\nlog_probs = [lp for _, lp in sequences]\nmost_probable_idx = max(range(len(log_probs)), key=lambda i: log_probs[i])\nmost_probable_text = sequences[most_probable_idx][0]\nprint(f\"\\nMost probable sequence: {most_probable_text}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create bump plot visualization\nfig = viz.visualize_bumplot(\n    particles=particles,\n    output_path=None,\n    max_vocab_display=15,\n    color_by='transition_prob',\n    show_tokens=True,\n    curve_force=0.5,\n    prompt=example_prompt\n)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Visualizations for All Starter Sequences\\n",
    "\\n",
    "Now let's generate visualizations for all our starter sequences, organizing them by ambiguity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create output directory\noutput_dir = \"../../data/derivatives/particle_visualizations\"\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"Output directory created at: {output_dir}\")\n\n# Categories for organization\ncategories = {\n    \"high_ambiguity\": starter_sequences[0:5],\n    \"medium_ambiguity\": starter_sequences[5:10],\n    \"low_ambiguity\": starter_sequences[10:15],\n    \"story_beginnings\": starter_sequences[15:20],\n    \"philosophical\": starter_sequences[20:25],\n    \"technical\": starter_sequences[25:30]\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate visualizations for each category\n# We'll process only the first prompt from each category for demonstration\nresults = {}\n\nfor category, prompts in categories.items():\n    print(f\"\\n=== Processing {category} ===\")\n    category_dir = os.path.join(output_dir, category)\n    os.makedirs(category_dir, exist_ok=True)\n    \n    results[category] = []\n    \n    # Process only first prompt from each category\n    for i, prompt in enumerate(prompts[:1]):\n        print(f\"Generating 1000 particles for: {prompt[:30]}...\")\n        \n        # Generate particles with 20 tokens\n        particles = pf.generate(prompt, max_new_tokens=20)\n        \n        # Save results\n        result = {\n            'prompt': prompt,\n            'sequences': pf.get_token_sequences(),\n            'particles': particles\n        }\n        results[category].append(result)\n        \n        # Create safe filename\n        safe_prompt = prompt.replace(' ', '_').replace('/', '_').replace('\\\\n', '_')[:30]\n        \n        # Generate Sankey diagram\n        fig = viz.visualize(\n            particles=particles,\n            prompt=prompt,\n            output_path=os.path.join(category_dir, f\"{i:02d}_sankey_{safe_prompt}.png\")\n        )\n        plt.close(fig)\n        print(f\"  ✓ Generated Sankey diagram\")\n        \n        # Generate heatmap\n        fig = viz.visualize_probability_heatmap(\n            particles=particles,\n            prompt=prompt,\n            vocab_size=32000,\n            output_path=os.path.join(category_dir, f\"{i:02d}_heatmap_{safe_prompt}.png\")\n        )\n        plt.close(fig)\n        print(f\"  ✓ Generated heatmap\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Generate visualizations for each category\n# We'll process only the first prompt from each category for demonstration\nresults = {}\n\nfor category, prompts in categories.items():\n    print(f\"\\n=== Processing {category} ===\")\n    category_dir = os.path.join(output_dir, category)\n    os.makedirs(category_dir, exist_ok=True)\n    \n    results[category] = []\n    \n    # Process only first prompt from each category\n    for i, prompt in enumerate(prompts[:1]):\n        print(f\"Generating 1000 particles for: {prompt[:30]}...\")\n        \n        # Generate particles with 20 tokens\n        particles = pf.generate(prompt, max_new_tokens=20)\n        \n        # Save results\n        result = {\n            'prompt': prompt,\n            'sequences': pf.get_token_sequences(),\n            'particles': particles\n        }\n        results[category].append(result)\n        \n        # Create safe filename\n        safe_prompt = prompt.replace(' ', '_').replace('/', '_').replace('\\\\n', '_')[:30]\n        \n        # Generate bump plot visualization\n        fig = viz.visualize_bumplot(\n            particles=particles,\n            output_path=os.path.join(category_dir, f\"{i:02d}_bumplot_{safe_prompt}.png\"),\n            max_vocab_display=15,\n            color_by='transition_prob',\n            show_tokens=True,\n            prompt=prompt\n        )\n        plt.close(fig)\n        print(f\"  ✓ Generated bump plot visualization\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate divergence metrics\\n",
    "def calculate_divergence_metrics(particles):\\n",
    "    \\\"\\\"\\\"Calculate metrics for path divergence.\\\"\\\"\\\"\\n",
    "    sequences = [p.tokens for p in particles]\\n",
    "    n_particles = len(sequences)\\n",
    "    \\n",
    "    if n_particles < 2:\\n",
    "        return {'avg_divergence_point': 0, 'final_diversity': 0}\\n",
    "    \\n",
    "    # Find average divergence point\\n",
    "    min_len = min(len(seq) for seq in sequences)\\n",
    "    divergence_points = []\\n",
    "    \\n",
    "    for i in range(n_particles):\\n",
    "        for j in range(i + 1, n_particles):\\n",
    "            # Find where sequences diverge\\n",
    "            for k in range(min_len):\\n",
    "                if sequences[i][k] != sequences[j][k]:\\n",
    "                    divergence_points.append(k)\\n",
    "                    break\\n",
    "    \\n",
    "    avg_divergence = np.mean(divergence_points) if divergence_points else min_len\\n",
    "    \\n",
    "    # Calculate final diversity (unique endings)\\n",
    "    final_tokens = [tuple(seq[-5:]) for seq in sequences if len(seq) >= 5]\\n",
    "    final_diversity = len(set(final_tokens)) / n_particles if final_tokens else 0\\n",
    "    \\n",
    "    return {\\n",
    "        'avg_divergence_point': avg_divergence,\\n",
    "        'final_diversity': final_diversity\\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze divergence by category\\n",
    "divergence_analysis = {}\\n",
    "\\n",
    "for category, category_results in results.items():\\n",
    "    metrics = []\\n",
    "    for result in category_results:\\n",
    "        metric = calculate_divergence_metrics(result['particles'])\\n",
    "        metrics.append(metric)\\n",
    "    \\n",
    "    divergence_analysis[category] = {\\n",
    "        'avg_divergence_point': np.mean([m['avg_divergence_point'] for m in metrics]),\\n",
    "        'avg_final_diversity': np.mean([m['final_diversity'] for m in metrics])\\n",
    "    }\\n",
    "\\n",
    "# Create visualization\\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n",
    "\\n",
    "categories_ordered = list(divergence_analysis.keys())\\n",
    "divergence_points = [divergence_analysis[c]['avg_divergence_point'] for c in categories_ordered]\\n",
    "diversities = [divergence_analysis[c]['avg_final_diversity'] for c in categories_ordered]\\n",
    "\\n",
    "# Plot average divergence point\\n",
    "ax1.bar(range(len(categories_ordered)), divergence_points, color='skyblue')\\n",
    "ax1.set_xticks(range(len(categories_ordered)))\\n",
    "ax1.set_xticklabels(categories_ordered, rotation=45, ha='right')\\n",
    "ax1.set_ylabel('Average Divergence Point (tokens)')\\n",
    "ax1.set_title('When Do Paths Diverge?')\\n",
    "ax1.grid(True, alpha=0.3)\\n",
    "\\n",
    "# Plot final diversity\\n",
    "ax2.bar(range(len(categories_ordered)), diversities, color='lightcoral')\\n",
    "ax2.set_xticks(range(len(categories_ordered)))\\n",
    "ax2.set_xticklabels(categories_ordered, rotation=45, ha='right')\\n",
    "ax2.set_ylabel('Final Diversity Score')\\n",
    "ax2.set_title('How Different Are Final Outputs?')\\n",
    "ax2.grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.savefig(os.path.join(output_dir, 'divergence_analysis.png'), dpi=300, bbox_inches='tight')\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exploration\\n",
    "\\n",
    "Try your own prompts to see how the model explores different paths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt exploration\\n",
    "def explore_prompt(prompt, n_particles=10, max_tokens=30, temperature=0.8):\\n",
    "    \\\"\\\"\\\"Explore a custom prompt with particle filtering.\\\"\\\"\\\"\\n",
    "    # Update particle filter settings\\n",
    "    pf.n_particles = n_particles\\n",
    "    pf.temperature = temperature\\n",
    "    \\n",
    "    # Generate\\n",
    "    print(f\\\"Generating {n_particles} particles for: '{prompt}'\\\")\\n",
    "    particles = pf.generate(prompt, max_new_tokens=max_tokens)\\n",
    "    \\n",
    "    # Show sequences\\n",
    "    print(\\\"\\\\nGenerated sequences:\\\")\\n",
    "    for i, (text, log_prob) in enumerate(pf.get_token_sequences()):\\n",
    "        print(f\\\"\\\\nParticle {i+1}:\\\")\\n",
    "        print(f\\\"  Text: {text}\\\")\\n",
    "        print(f\\\"  Log prob: {log_prob:.3f}\\\")\\n",
    "    \\n",
    "    # Visualize\\n",
    "    fig = viz.visualize(particles=particles, prompt=prompt)\\n",
    "    plt.show()\\n",
    "    \\n",
    "    return particles\\n",
    "\\n",
    "# Example usage\\n",
    "custom_particles = explore_prompt(\\n",
    "    \\\"The future of AI will \\\",\\n",
    "    n_particles=8,\\n",
    "    max_tokens=25,\\n",
    "    temperature=1.0\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Interactive prompt exploration\ndef explore_prompt(prompt, n_particles=10, max_tokens=30, temperature=0.8):\n    \"\"\"Explore a custom prompt with particle filtering.\"\"\"\n    # Update particle filter settings\n    pf.n_particles = n_particles\n    pf.temperature = temperature\n    \n    # Generate\n    print(f\"Generating {n_particles} particles for: '{prompt}'\")\n    particles = pf.generate(prompt, max_new_tokens=max_tokens)\n    \n    # Show sequences\n    print(\"\\nGenerated sequences:\")\n    for i, (text, log_prob) in enumerate(pf.get_token_sequences()):\n        print(f\"\\nParticle {i+1}:\")\n        print(f\"  Text: {text}\")\n        print(f\"  Log prob: {log_prob:.3f}\")\n    \n    # Visualize with bump plot\n    fig = viz.visualize_bumplot(\n        particles=particles,\n        max_vocab_display=15,\n        color_by='transition_prob',\n        show_tokens=True,\n        prompt=prompt\n    )\n    plt.show()\n    \n    return particles\n\n# Example usage\ncustom_particles = explore_prompt(\n    \"The future of AI will \",\n    n_particles=8,\n    max_tokens=25,\n    temperature=1.0\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
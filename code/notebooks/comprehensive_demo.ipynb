{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Conversations: Comprehensive Demo\n",
    "\n",
    "This notebook demonstrates the complete Quantum Conversations toolkit, including particle filter generation, multiple visualization types, and analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from quantum_conversations import (\n",
    "    ParticleFilter, \n",
    "    TokenSequenceVisualizer,\n",
    "    ModelManager,\n",
    "    save_particles,\n",
    "    load_particles,\n",
    "    compute_sequence_entropy,\n",
    "    compute_divergence_score,\n",
    "    create_probability_tensor,\n",
    "    get_top_tokens\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../outputs/demo_figures')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Management\n",
    "\n",
    "Demonstrate the flexible model management system that supports any open-weights HuggingFace model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Load different models to show flexibility\n",
    "models_to_test = [\n",
    "    \"EleutherAI/pythia-70m\",  # Small model for quick testing\n",
    "    \"gpt2\",                    # Classic GPT-2\n",
    "]\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nLoading {model_name}...\")\n",
    "    model, tokenizer = model_manager.load_model(model_name, device=\"cpu\")\n",
    "    print(f\"  ✓ Model loaded: {model.__class__.__name__}\")\n",
    "    print(f\"  ✓ Vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"  ✓ Cached: {model_name in model_manager.model_cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Particle Filter Generation\n",
    "\n",
    "Generate particles with different configurations to explore the token probability space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompts and temperatures\n",
    "test_configs = [\n",
    "    {\"prompt\": \"The future of AI is\", \"temp\": 0.5, \"label\": \"Low Temperature (0.5)\"},\n",
    "    {\"prompt\": \"The future of AI is\", \"temp\": 1.0, \"label\": \"Medium Temperature (1.0)\"},\n",
    "    {\"prompt\": \"The future of AI is\", \"temp\": 1.5, \"label\": \"High Temperature (1.5)\"},\n",
    "]\n",
    "\n",
    "all_particles = {}\n",
    "\n",
    "for config in test_configs:\n",
    "    pf = ParticleFilter(\n",
    "        model_name=\"EleutherAI/pythia-70m\",\n",
    "        n_particles=8,\n",
    "        temperature=config[\"temp\"],\n",
    "        device=\"cpu\",\n",
    "        model_manager=model_manager,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    particles = pf.generate(config[\"prompt\"], max_new_tokens=15)\n",
    "    all_particles[config[\"label\"]] = (particles, pf.tokenizer)\n",
    "    \n",
    "    print(f\"\\n{config['label']}:\")\n",
    "    print(f\"  Generated {len(particles)} particles\")\n",
    "    print(f\"  Sample outputs:\")\n",
    "    for i, p in enumerate(particles[:3]):\n",
    "        text = pf.tokenizer.decode(p.token_ids, skip_special_tokens=True)\n",
    "        print(f\"    {i+1}. {text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Suite\n",
    "\n",
    "### 3.1 Bumplot Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bumplot for each temperature setting\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "for idx, (label, (particles, tokenizer)) in enumerate(all_particles.items()):\n",
    "    viz = TokenSequenceVisualizer(tokenizer=tokenizer)\n",
    "    \n",
    "    # Create bumplot in subplot\n",
    "    plt.sca(axes[idx])\n",
    "    viz.visualize_bumplot(\n",
    "        particles,\n",
    "        color_by='transition_prob',\n",
    "        max_vocab_display=25,\n",
    "        show_tokens=False,\n",
    "        figsize=None  # Use existing figure\n",
    "    )\n",
    "    axes[idx].set_title(f\"Bumplot: {label}\")\n",
    "\n",
    "plt.suptitle(\"Token Trajectory Comparison Across Temperatures\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"bumplot_temperature_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved: {output_dir / 'bumplot_temperature_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sankey Diagram Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate particles for Sankey visualization\n",
    "pf_sankey = ParticleFilter(\n",
    "    model_name=\"EleutherAI/pythia-70m\",\n",
    "    n_particles=10,\n",
    "    temperature=1.0,\n",
    "    device=\"cpu\",\n",
    "    model_manager=model_manager,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "prompt_sankey = \"Once upon a time\"\n",
    "particles_sankey = pf_sankey.generate(prompt_sankey, max_new_tokens=12)\n",
    "\n",
    "viz_sankey = TokenSequenceVisualizer(tokenizer=pf_sankey.tokenizer)\n",
    "fig = viz_sankey.visualize(\n",
    "    particles_sankey,\n",
    "    prompt_sankey,\n",
    "    figsize=(14, 8),\n",
    "    output_path=str(output_dir / \"sankey_diagram.png\")\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved: {output_dir / 'sankey_diagram.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Probability Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability heatmap\n",
    "fig = viz_sankey.visualize_heatmap(\n",
    "    particles_sankey,\n",
    "    figsize=(12, 8),\n",
    "    output_path=str(output_dir / \"probability_heatmap.png\")\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved: {output_dir / 'probability_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis Tools\n",
    "\n",
    "### 4.1 Entropy and Divergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entropy and divergence for different temperatures\n",
    "analysis_results = {}\n",
    "\n",
    "for label, (particles, tokenizer) in all_particles.items():\n",
    "    entropies = [compute_sequence_entropy(p) for p in particles]\n",
    "    divergence = compute_divergence_score(particles)\n",
    "    \n",
    "    analysis_results[label] = {\n",
    "        'mean_entropy': np.mean(entropies),\n",
    "        'std_entropy': np.std(entropies),\n",
    "        'divergence': divergence\n",
    "    }\n",
    "\n",
    "# Plot analysis results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Entropy plot\n",
    "labels = list(analysis_results.keys())\n",
    "mean_entropies = [analysis_results[l]['mean_entropy'] for l in labels]\n",
    "std_entropies = [analysis_results[l]['std_entropy'] for l in labels]\n",
    "\n",
    "x_pos = np.arange(len(labels))\n",
    "ax1.bar(x_pos, mean_entropies, yerr=std_entropies, capsize=10, alpha=0.7)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Mean Entropy')\n",
    "ax1.set_title('Sequence Entropy by Temperature')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Divergence plot\n",
    "divergences = [analysis_results[l]['divergence'] for l in labels]\n",
    "ax2.bar(x_pos, divergences, alpha=0.7, color='orange')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Divergence Score')\n",
    "ax2.set_title('Path Divergence by Temperature')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"entropy_divergence_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved: {output_dir / 'entropy_divergence_analysis.png'}\")\n",
    "print(\"\\nAnalysis Results:\")\n",
    "for label, results in analysis_results.items():\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Mean Entropy: {results['mean_entropy']:.4f} ± {results['std_entropy']:.4f}\")\n",
    "    print(f\"  Divergence Score: {results['divergence']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Probability Tensor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and analyze probability tensor\n",
    "particles_for_tensor, tokenizer_for_tensor = all_particles[\"Medium Temperature (1.0)\"]\n",
    "\n",
    "# Create probability tensor\n",
    "tensor = create_probability_tensor(particles_for_tensor)\n",
    "print(f\"Probability tensor shape: {tensor.shape}\")\n",
    "print(f\"  (vocab_size, timesteps, n_particles) = {tensor.shape}\")\n",
    "\n",
    "# Get top tokens at each timestep\n",
    "for t in range(min(5, tensor.shape[1])):\n",
    "    top_tokens = get_top_tokens(particles_for_tensor, timestep=t, k=5)\n",
    "    print(f\"\\nTimestep {t} - Top 5 tokens:\")\n",
    "    for token_id, prob in top_tokens:\n",
    "        token_text = tokenizer_for_tensor.decode([token_id])\n",
    "        print(f\"  '{token_text}': {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persistence: Save and Load Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save particles\n",
    "save_path = output_dir / \"saved_particles.pkl\"\n",
    "particles_to_save, tokenizer_to_save = all_particles[\"High Temperature (1.5)\"]\n",
    "save_particles(particles_to_save, save_path)\n",
    "print(f\"Saved {len(particles_to_save)} particles to {save_path}\")\n",
    "\n",
    "# Load particles\n",
    "loaded_particles = load_particles(save_path)\n",
    "print(f\"\\nLoaded {len(loaded_particles)} particles from {save_path}\")\n",
    "\n",
    "# Verify loaded particles\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"  Original first particle tokens: {particles_to_save[0].token_ids[:5]}\")\n",
    "print(f\"  Loaded first particle tokens: {loaded_particles[0].token_ids[:5]}\")\n",
    "print(f\"  ✓ Particles successfully saved and loaded\" if \n",
    "      particles_to_save[0].token_ids == loaded_particles[0].token_ids else \n",
    "      \"  ✗ Error in save/load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Different Model Sizes Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model sizes\n",
    "models_comparison = {\n",
    "    \"Pythia-70M\": \"EleutherAI/pythia-70m\",\n",
    "    \"GPT-2 (124M)\": \"gpt2\"\n",
    "}\n",
    "\n",
    "prompt_comparison = \"The secret to happiness is\"\n",
    "comparison_particles = {}\n",
    "\n",
    "for model_label, model_name in models_comparison.items():\n",
    "    print(f\"\\nGenerating with {model_label}...\")\n",
    "    pf = ParticleFilter(\n",
    "        model_name=model_name,\n",
    "        n_particles=6,\n",
    "        temperature=1.0,\n",
    "        device=\"cpu\",\n",
    "        model_manager=model_manager,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    particles = pf.generate(prompt_comparison, max_new_tokens=15)\n",
    "    comparison_particles[model_label] = (particles, pf.tokenizer)\n",
    "    \n",
    "    # Show sample outputs\n",
    "    print(f\"  Sample outputs from {model_label}:\")\n",
    "    for i, p in enumerate(particles[:2]):\n",
    "        text = pf.tokenizer.decode(p.token_ids, skip_special_tokens=True)\n",
    "        print(f\"    {text[:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison with bumplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, (model_label, (particles, tokenizer)) in enumerate(comparison_particles.items()):\n",
    "    viz = TokenSequenceVisualizer(tokenizer=tokenizer)\n",
    "    plt.sca(axes[idx])\n",
    "    viz.visualize_bumplot(\n",
    "        particles,\n",
    "        color_by='particle_id',\n",
    "        max_vocab_display=20,\n",
    "        show_tokens=False,\n",
    "        figsize=None\n",
    "    )\n",
    "    axes[idx].set_title(f\"{model_label}\\nPrompt: '{prompt_comparison}'\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"model_comparison_bumplot.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved: {output_dir / 'model_comparison_bumplot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Exploration\n",
    "\n",
    "Try your own prompts and configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation function\n",
    "def explore_prompt(prompt, model_name=\"EleutherAI/pythia-70m\", n_particles=8, \n",
    "                   temperature=1.0, max_tokens=15):\n",
    "    \"\"\"Generate and visualize particles for a custom prompt.\"\"\"\n",
    "    \n",
    "    print(f\"\\nExploring: '{prompt}'\")\n",
    "    print(f\"Settings: model={model_name}, particles={n_particles}, temp={temperature}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Generate particles\n",
    "    pf = ParticleFilter(\n",
    "        model_name=model_name,\n",
    "        n_particles=n_particles,\n",
    "        temperature=temperature,\n",
    "        device=\"cpu\",\n",
    "        model_manager=model_manager,\n",
    "        seed=None  # Random seed for variety\n",
    "    )\n",
    "    \n",
    "    particles = pf.generate(prompt, max_new_tokens=max_tokens)\n",
    "    \n",
    "    # Show outputs\n",
    "    print(\"\\nGenerated sequences:\")\n",
    "    for i, p in enumerate(particles):\n",
    "        text = pf.tokenizer.decode(p.token_ids, skip_special_tokens=True)\n",
    "        print(f\"  {i+1}. {text}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    viz = TokenSequenceVisualizer(tokenizer=pf.tokenizer)\n",
    "    \n",
    "    # Bumplot\n",
    "    fig = viz.visualize_bumplot(\n",
    "        particles,\n",
    "        color_by='transition_prob',\n",
    "        max_vocab_display=30,\n",
    "        show_tokens=False,\n",
    "        figsize=(14, 6)\n",
    "    )\n",
    "    plt.title(f\"Token Trajectories: '{prompt}'\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute metrics\n",
    "    divergence = compute_divergence_score(particles)\n",
    "    entropies = [compute_sequence_entropy(p) for p in particles]\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Divergence score: {divergence:.4f}\")\n",
    "    print(f\"  Mean entropy: {np.mean(entropies):.4f}\")\n",
    "    \n",
    "    return particles\n",
    "\n",
    "# Example usage\n",
    "custom_particles = explore_prompt(\n",
    "    \"In a world where\",\n",
    "    temperature=1.2,\n",
    "    n_particles=8,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prompt types\n",
    "interesting_prompts = [\n",
    "    \"The most important thing in life is\",\n",
    "    \"def fibonacci(n):\",\n",
    "    \"Breaking news:\",\n",
    "    \"To be or not to be,\"\n",
    "]\n",
    "\n",
    "for prompt in interesting_prompts:\n",
    "    particles = explore_prompt(prompt, temperature=1.0, n_particles=6, max_tokens=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive demo showcased:\n",
    "\n",
    "1. **Model Management**: Flexible loading of any open-weights HuggingFace model\n",
    "2. **Particle Generation**: Multiple hypothesis tracking with configurable parameters\n",
    "3. **Visualization Suite**:\n",
    "   - Bumplot visualization for token trajectories\n",
    "   - Sankey diagrams for path visualization\n",
    "   - Probability heatmaps\n",
    "4. **Analysis Tools**: Entropy and divergence metrics\n",
    "5. **Probability Tensors**: V×t×n tensor creation and analysis\n",
    "6. **Persistence**: Save/load functionality for particles\n",
    "7. **Model Comparison**: Comparing behavior across different model sizes\n",
    "8. **Interactive Exploration**: Custom prompt analysis\n",
    "\n",
    "All visualizations and outputs have been saved to: `code/outputs/demo_figures/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}